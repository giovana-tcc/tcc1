Para o Passo 4, descrito na Seção \ref{subsec:tarefa}, será considerada a abordagem de \emph{Transfer Learning}, visando aproveitar parâmetros de redes CNNs pré-treinadas com \emph{datasets} massivos, especialmente o ImageNet. Para tanto, considerando a utilização do \emph{framework} \texttt{keras}, foi consultada a biblioteca de modelos previamente treinados, disponíveis no módulo \texttt{applications}. Como resultado, foram identificadas as arquiteturas canônicas e suas características elencadas na Tabela \ref{tab:cnns}.

\begin{table}[!ht]
	\caption{Arquiteturas canônicas e suas características disponíveis no módulo \texttt{keras.applications}.}
	\centering
	\begin{tabular}{c c c c}
		\toprule
		 Arquitetura & Profundidade & Parâmetros & Tamanho \\
		\midrule
		Xception & 126 & $22,910,480$ & 88 MB \\
		VGG16 & 23 & $138,357,544$ & 528 MB \\
		VGG19 & 26 & $143,667,240$ & 549 MB \\
		ResNet50 & 168 & $25,636,712$ & 99 MB \\
		InceptionV3 & 159 & $23,851,784$ & 92 MB \\
		InceptionResNetV2 & 572 & $55,873,736$ & 215 MB \\
		MobileNet & 88 & $4,253,864$ & 17 MB \\
		\bottomrule
	\end{tabular}

	\label{tab:cnns}
\end{table}

É interessante notar que todos estes modelos disponíveis foram pré-treinados com a base de dados do ImageNet. Considerando a quantidade de imagens e a variedade desses exemplos, acredita-se que estes pesos podem contribuir positivamente para o problema da coloração dada a transferência de aprendizado.

Levando em conta os recursos computacionais disponíveis em nuvem para este projeto e o tempo de processamento, duas CNNs, a VGG16 e ResNet50, disponíveis no \texttt{keras.applications} serão ajustadas perante \emph{Transfer Learning} para a tarefa de coloração proposta. Havendo tempo posterior disponível, outras arquiteturas também serão contempladas.

Dessas redes pré-treinadas, as últimas camadas serão substituídas por camadas completamente conectadas com saída compatível com as matrizes $a$ e $b$ de coloração. Eventuais re-treinos serão efetuados para ajuste de interconexão dessa nova camada. Serão consideradas $100$ épocas para o treinamento dos modelos propostos e os demais parâmetros terão seus valores padrões preservados.
